{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 건축과 AI 수업자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pl_bolts.optimizers.lars_scheduling import LARSWrapper\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리를 통한 효율적인 코드 작성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_path = '../../Share_Data/crack_segmentation_dataset/train/'\n",
    "testset_path = '../../Share_Data/crack_segmentation_dataset/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_test_dataset(Dataset):\n",
    "    def __init__(self, data_path, folder_name_list = ['images', 'masks']):\n",
    "        super(my_test_dataset, self).__init__()\n",
    "        self.img_name_list = [img_f_name[:-4] for img_f_name in listdir(data_path+'images')]\n",
    "        self.img_path = data_path + folder_name_list[0]+'/'\n",
    "        self.mask_path = data_path + folder_name_list[1]+'/'\n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "        self.mask_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self), 'wrong index'\n",
    "        img_name = self.img_name_list[idx]\n",
    "        return (self.img_transforms(Image.open(self.img_path+img_name+'.jpg')), \n",
    "               self.mask_transforms(Image.open(self.mask_path+img_name+'.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision's FCN_Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "WARM_EPOCH = 10\n",
    "MAX_EPOCH = 3000\n",
    "\n",
    "trainset_path = '../../Share_Data/crack_segmentation_dataset/train/'\n",
    "testset_path = '../../Share_Data/crack_segmentation_dataset/test/'\n",
    "\n",
    "\n",
    "class my_module(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(my_module, self).__init__()\n",
    "        self.my_model = models.segmentation.fcn_resnet50(num_classes=1)\n",
    "        self.loss_f = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        return self.my_model(x)['out']\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_f(y, y_hat)\n",
    "        self.logger.experiment.log({'training_loss':loss})\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_f(y, y_hat)\n",
    "        return loss\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(outputs).mean()\n",
    "        self.logger.experiment.log({'avg_val_loss' : avg_loss})\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        my_opt = LARSWrapper(Adam(self.my_model.parameters(), lr=LR))\n",
    "        my_sch = LinearWarmupCosineAnnealingLR(\n",
    "            my_opt,\n",
    "            warmup_epochs=WARM_EPOCH,\n",
    "            max_epochs=MAX_EPOCH)\n",
    "        rst_sch = {\n",
    "            'scheduler' : my_sch,\n",
    "            'interval': 'epoch',\n",
    "            'frequency' : 1\n",
    "        }\n",
    "        return [my_opt], [rst_sch]\n",
    "\n",
    "class my_dataset(Dataset):\n",
    "    def __init__(self, data_path, folder_name_list = ['images', 'masks']):\n",
    "        super(my_dataset, self).__init__()\n",
    "        self.img_name_list = [img_f_name[:-4] for img_f_name in listdir(data_path+'images')]\n",
    "        self.img_path = data_path + folder_name_list[0]+'/'\n",
    "        self.mask_path = data_path + folder_name_list[1]+'/'\n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "        self.mask_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self), 'wrong index'\n",
    "        img_name = self.img_name_list[idx]\n",
    "        return (self.img_transforms(Image.open(self.img_path+img_name+'.jpg')), \n",
    "               self.mask_transforms(Image.open(self.mask_path+img_name+'.jpg')))\n",
    "    \n",
    "    \n",
    "class my_datamodule(pl.LightningDataModule):\n",
    "    def __init__(self, trainset_path, testset_path):\n",
    "        super(my_datamodule, self).__init__()\n",
    "        self.trainset_path = trainset_path\n",
    "        self.testset_path = testset_path\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_dataset = my_dataset(self.trainset_path)\n",
    "        return DataLoader(train_dataset, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = my_dataset(self.testset_path)\n",
    "        return DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "        \n",
    "\n",
    "class my_test_dataset(Dataset):\n",
    "    def __init__(self, data_path, folder_name_list = ['images', 'masks']):\n",
    "        super(my_test_dataset, self).__init__()\n",
    "        self.img_name_list = [img_f_name[:-4] for img_f_name in listdir(data_path+'images')]\n",
    "        self.img_path = data_path + folder_name_list[0]+'/'\n",
    "        self.mask_path = data_path + folder_name_list[1]+'/'\n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "        self.mask_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_name_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self), 'wrong index'\n",
    "        img_name = self.img_name_list[idx]\n",
    "        return (self.img_transforms(Image.open(self.img_path+img_name+'.jpg')), \n",
    "               self.mask_transforms(Image.open(self.mask_path+img_name+'.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "\n                You requested GPUs: [0]\n                But your machine only has: []\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5600fdc57a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                    \u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_callback_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                    \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                    max_epochs=100)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\u001b[0m in \u001b[0;36moverwrite_by_env_vars\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moverwrite_by_env_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, distributed_backend, automatic_optimization)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mreplace_sampler_ddp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         )\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator_connector.py\u001b[0m in \u001b[0;36mon_trainer_init\u001b[0;34m(self, num_processes, tpu_cores, accelerator, distributed_backend, auto_select_gpus, gpus, num_nodes, log_gpu_memory, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpick_multiple_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetermine_root_gpu_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPUs requested but none are available.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_gpu_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch16/lib/python3.6/site-packages/pytorch_lightning/utilities/device_parser.py\u001b[0m in \u001b[0;36m_sanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mYou\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mGPUs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mBut\u001b[0m \u001b[0myour\u001b[0m \u001b[0mmachine\u001b[0m \u001b[0monly\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mall_available_gpus\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \"\"\")\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: \n                You requested GPUs: [0]\n                But your machine only has: []\n            "
     ]
    }
   ],
   "source": [
    "SAVE_DIR = 'logs/'\n",
    "MODEL_NAME = 'CRACK'\n",
    "\n",
    "checkpoint_callback_valid = pl.callbacks.ModelCheckpoint(monitor='avg_val_loss',\n",
    "                                                        save_last = True,\n",
    "                                                        save_top_k = 1,\n",
    "                                                        mode='min')\n",
    "\n",
    "tt_logger = pl.loggers.TestTubeLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name=MODEL_NAME,\n",
    "    debug=False,\n",
    "    create_git_tag=False)\n",
    "\n",
    "crack_datamodule = my_datamodule(trainset_path=trainset_path, testset_path=testset_path)\n",
    "crack_model = my_module()\n",
    "\n",
    "runner = pl.Trainer(default_root_dir=f\"{tt_logger.save_dir}\",\n",
    "                   logger=tt_logger,\n",
    "                   checkpoint_callback = checkpoint_callback_valid,\n",
    "                   gpus=1,\n",
    "                   max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch16",
   "language": "python",
   "name": "pytorch16"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
